{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-935f168efdb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#from vis import plot_embeddings, plot_kls_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#from .vae import VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/proj/multimodal/src/models/vae.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membed_umap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors_to_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# ATAC model specification\n",
    "\n",
    "#MMVAE module\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import prod, sqrt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from utils import Constants\n",
    "from vis import plot_embeddings, plot_kls_df\n",
    "from .vae import VAE\n",
    "\n",
    "#SCALE module\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from torch.nn import init\n",
    "#from torch.optim.lr_scheduler import MultiStepLR, ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "#import time\n",
    "#import math\n",
    "#import numpy as np\n",
    "#from tqdm import trange\n",
    "#from itertools import repeat\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#from .layer import Encoder, Decoder, build_mlp, DeterministicWarmup\n",
    "#from .loss import elbo, elbo_SCALE\n",
    "\n",
    "from datasets import SingleCellDataset\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataSize = torch.Size([43703,1]) #number of filtered peaks p0 Brain Cortex SNARE-seq\n",
    "data_dim = int(prod(dataSize))\n",
    "hidden_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_hidden_layer():\n",
    "    return nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "class Enc(nn.Module):\n",
    "    \"\"\" Generate latent parameters for ATAC-seq data. \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, num_hidden_layers=1):\n",
    "        super(Enc, self).__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(nn.Linear(data_dim, hidden_dim), nn.ReLU(True)))\n",
    "        modules.extend([extra_hidden_layer() for _ in range(num_hidden_layers - 1)])\n",
    "        self.enc = nn.Sequential(*modules)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #e = self.enc(x.view(*x.size()[:-3], -1))  # flatten data\n",
    "        e = self.enc(x)\n",
    "        lv = self.fc22(e)\n",
    "        return self.fc21(e), F.softmax(lv, dim=-1) * lv.size(-1) + Constants.eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enc = Enc(20,1)\n",
    "Enc.enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATAC(VAE):\n",
    "    \"\"\" Derive a specific sub-class of a VAE for ATAC. \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(ATAC, self).__init__(\n",
    "            dist.Laplace,  # prior\n",
    "            dist.Laplace,  # likelihood\n",
    "            dist.Laplace,  # posterior\n",
    "            Enc(params.latent_dim, params.num_hidden_layers),\n",
    "            Dec(params.latent_dim, params.num_hidden_layers),\n",
    "            params\n",
    "        )\n",
    "        grad = {'requires_grad': params.learn_prior}\n",
    "        self._pz_params = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(1, params.latent_dim), requires_grad=False),  # mu\n",
    "            nn.Parameter(torch.zeros(1, params.latent_dim), **grad)  # logvar\n",
    "        ])\n",
    "        self.modelName = 'atac'\n",
    "        self.dataSize = dataSize\n",
    "        self.llik_scaling = 1.\n",
    "\n",
    "    @property\n",
    "    def pz_params(self):\n",
    "        return self._pz_params[0], F.softmax(self._pz_params[1], dim=1) * self._pz_params[1].size(-1)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDataLoaders(batch_size, shuffle=True, device=\"cuda\"):\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True} if device == \"cuda\" else {}\n",
    "        \n",
    "        #SingleCellDatasetを移植\n",
    "        path = '../data/'\n",
    "        #batch_size = 32\n",
    "        low = 0.01\n",
    "        high = 0.9\n",
    "        min_peaks = 100\n",
    "        transpose = False \n",
    "        normalizer = MaxAbsScaler()\n",
    "        \n",
    "        dataset = SingleCellDataset(path, low=low, high=high, min_peaks=min_peaks,\n",
    "                            transpose=transpose, transforms=[normalizer.fit_transform])\n",
    "        \n",
    "        train = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "        test = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False, **kwargs)\n",
    "        \n",
    "        #tx = transforms.ToTensor()\n",
    "        #train = DataLoader(datasets.MNIST('../data', train=True, download=True, transform=tx),\n",
    "        #                  batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "        #test = DataLoader(datasets.MNIST('../data', train=False, download=True, transform=tx),\n",
    "        #                  batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "        \n",
    "        \n",
    "        return train, test \n",
    "\n",
    "    def generate(self, runPath, epoch):\n",
    "        N, K = 64, 9\n",
    "        samples = super(MNIST, self).generate(N, K).cpu()\n",
    "        # wrangle things so they come out tiled\n",
    "        samples = samples.view(K, N, *samples.size()[1:]).transpose(0, 1)  # N x K x 1 x 28 x 28\n",
    "        s = [make_grid(t, nrow=int(sqrt(K)), padding=0) for t in samples]\n",
    "        save_image(torch.stack(s),\n",
    "                   '{}/gen_samples_{:03d}.png'.format(runPath, epoch),\n",
    "                   nrow=int(sqrt(N)))\n",
    "\n",
    "    def reconstruct(self, data, runPath, epoch):\n",
    "        recon = super(MNIST, self).reconstruct(data[:8])\n",
    "        comp = torch.cat([data[:8], recon]).data.cpu()\n",
    "        save_image(comp, '{}/recon_{:03d}.png'.format(runPath, epoch))\n",
    "\n",
    "    def analyse(self, data, runPath, epoch):\n",
    "        zemb, zsl, kls_df = super(MNIST, self).analyse(data, K=10)\n",
    "        labels = ['Prior', self.modelName.lower()]\n",
    "        plot_embeddings(zemb, zsl, labels, '{}/emb_umap_{:03d}.png'.format(runPath, epoch))\n",
    "        plot_kls_df(kls_df, '{}/kl_distance_{:03d}.png'.format(runPath, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_multimodal",
   "language": "python",
   "name": "venv_multimodal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
