{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import models\n",
    "import objectives\n",
    "from utils import Logger, Timer, save_model, save_vars, unpack_data\n",
    "\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#args\n",
    "experiment = ''\n",
    "model = 'atac' #VAE試しに使う\n",
    "obj = 'elbo'\n",
    "K = 20\n",
    "looser = False\n",
    "llik_scaling = 0\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "latent_dim = 20\n",
    "num_hidden_layers = 1\n",
    "learn_prior = False\n",
    "logp = False\n",
    "print_freq = 0\n",
    "no_analytics = True\n",
    "seed = 1\n",
    "\n",
    "class params():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 experiment,\n",
    "                 model,\n",
    "                 obj,\n",
    "                 K,\n",
    "                 looser,\n",
    "                 llik_scaling,\n",
    "                 batch_size,\n",
    "                 epochs,\n",
    "                 latent_dim,\n",
    "                 num_hidden_layers,\n",
    "                 learn_prior,\n",
    "                 logp,\n",
    "                 print_freq,\n",
    "                 no_analytics,\n",
    "                 seed):\n",
    "        \n",
    "        self.experiment = experiment\n",
    "        self.model = model\n",
    "        self.obj = obj\n",
    "        self.K = K\n",
    "        self.looser = looser\n",
    "        self.llik_scaling = llik_scaling\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.learn_prior = learn_prior\n",
    "        self.logp = logp\n",
    "        self.print_freq = print_freq\n",
    "        self.no_analytics = no_analytics\n",
    "        self.seed = seed\n",
    "        \n",
    "args = params(experiment,\n",
    "                model,\n",
    "                 obj,\n",
    "                 K,\n",
    "                 looser,\n",
    "                 llik_scaling,\n",
    "                 batch_size,\n",
    "                 epochs,\n",
    "                 latent_dim,\n",
    "                 num_hidden_layers,\n",
    "                 learn_prior,\n",
    "                 logp,\n",
    "                 print_freq,\n",
    "                 no_analytics,\n",
    "                 seed)\n",
    "\n",
    "# random seed\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load model\n",
    "modelC = getattr(models, 'VAE_{}'.format(args.model))\n",
    "model = modelC(args).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  data ...\n",
      "Original data contains 5081 cells x 229429 peaks\n",
      "Finished loading takes 0.39 min\n",
      "<function elbo at 0x13b97ab70>\n"
     ]
    }
   ],
   "source": [
    "# preparation for training\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                       lr=1e-3, amsgrad=True)\n",
    "train_loader, test_loader = model.getDataLoaders(args.batch_size, device=device)\n",
    "objective = getattr(objectives,\n",
    "                    ('m_' if hasattr(model, 'vaes') else '')\n",
    "                    + args.obj\n",
    "                    + ('_looser' if (args.looser and args.obj != 'elbo') else ''))\n",
    "t_objective = getattr(objectives, ('m_' if hasattr(model, 'vaes') else '') + 'iwae')\n",
    "\n",
    "print(objective)\n",
    "\n",
    "def train(epoch, agg):\n",
    "    model.train()\n",
    "    b_loss = 0\n",
    "    for i, dataT in enumerate(train_loader):\n",
    "        data = unpack_data(dataT, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -objective(model, data, K=args.K)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        b_loss += loss.item()\n",
    "        if args.print_freq > 0 and i % args.print_freq == 0:\n",
    "            print(\"iteration {:04d}: loss: {:6.3f}\".format(i, loss.item() / args.batch_size))\n",
    "    agg['train_loss'].append(b_loss / len(train_loader.dataset))\n",
    "    print('====> Epoch: {:03d} Train loss: {:.4f}'.format(epoch, agg['train_loss'][-1]))\n",
    "    \n",
    "    \n",
    "def test(epoch, agg):\n",
    "    model.eval()\n",
    "    b_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, dataT in enumerate(test_loader):\n",
    "            data = unpack_data(dataT, device=device)\n",
    "            loss = -t_objective(model, data, K=args.K)\n",
    "            b_loss += loss.item()\n",
    "#            if i == 0:\n",
    "#               model.reconstruct(data, runPath, epoch)\n",
    "#                if not args.no_analytics:\n",
    "#                    model.analyse(data, runPath, epoch)\n",
    "    agg['test_loss'].append(b_loss / len(test_loader.dataset))\n",
    "    print('====>             Test loss: {:.4f}'.format(agg['test_loss'][-1]))\n",
    "\n",
    "    \n",
    "def estimate_log_marginal(K):\n",
    "    \"\"\"Compute an IWAE estimate of the log-marginal likelihood of test data.\"\"\"\n",
    "    model.eval()\n",
    "    marginal_loglik = 0\n",
    "    with torch.no_grad():\n",
    "        for dataT in test_loader:\n",
    "            data = unpack_data(dataT, device=device)\n",
    "            marginal_loglik += -t_objective(model, data, K).item()\n",
    "\n",
    "    marginal_loglik /= len(test_loader.dataset)\n",
    "    print('Marginal Log Likelihood (IWAE, K = {}): {:.4f}'.format(K, marginal_loglik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 43703])\n",
      "tensor(-15803613., grad_fn=<NegBackward>)\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "-69573897.21875\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "b_loss = 0\n",
    "\n",
    "#1epoch のみ run\n",
    "for i, dataT in enumerate(train_loader):\n",
    "    data = unpack_data(dataT, device=device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -objective(model, data, K=args.K)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    b_loss += loss.item()\n",
    "\n",
    "print(data.shape)\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(b_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up run path\n",
    "#runId = datetime.datetime.now().isoformat()\n",
    "#experiment_dir = Path('../experiments/' + args.experiment)\n",
    "#experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "#runPath = mkdtemp(prefix=runId, dir=str(experiment_dir))\n",
    "#sys.stdout = Logger('{}/run.log'.format(runPath))\n",
    "#print('Expt:', runPath)\n",
    "#print('RunID:', runId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer('MM-VAE') as t:\n",
    "        agg = defaultdict(list)\n",
    "        \n",
    "#        for epoch in range(1, args.epochs + 1):\n",
    "#            train(epoch, agg)\n",
    "#            test(epoch, agg)\n",
    "#            save_model(model, runPath + '/model.rar')\n",
    "#            save_vars(agg, runPath + '/losses.rar')\n",
    "#            model.generate(runPath, epoch)\n",
    "#        if args.logp:  # compute as tight a marginal likelihood as possible\n",
    "#            estimate_log_marginal(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace(loc: torch.Size([256, 20]), scale: torch.Size([256, 20]))\n"
     ]
    }
   ],
   "source": [
    "#class VAE のforward\n",
    "model._qz_x_params = model.enc(dataT) #Encoderを通して確率分布のパラメータを渡す\n",
    "qz_x = model.qz_x(*model._qz_x_params) #qz_x はLaplace分布\n",
    "print(qz_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs = qz_x.rsample(torch.Size([1])) #各サンプルについてqz_xからzをサンプリング\n",
    "zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06,\n",
      "          1.0000e-06, 1.0000e-06],\n",
      "         [1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06,\n",
      "          1.0000e-06, 1.0000e-06],\n",
      "         [1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06,\n",
      "          1.0000e-06, 1.0000e-06],\n",
      "         ...,\n",
      "         [1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06,\n",
      "          1.0000e-06, 1.0000e-06],\n",
      "         [1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06,\n",
      "          1.0000e-06, 1.0000e-06],\n",
      "         [1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06,\n",
      "          1.0000e-06, 1.0000e-06]]], grad_fn=<ClampBackward>), tensor(0.1000))\n",
      "torch.Size([1, 256, 43703])\n"
     ]
    }
   ],
   "source": [
    "recon = model.dec(zs)\n",
    "print(len(recon))\n",
    "print(recon[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace(loc: torch.Size([1, 256, 43703]), scale: torch.Size([1, 256, 43703]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_z = model.px_z(*model.dec(zs)) #likelihood?\n",
    "print(px_z)\n",
    "isinstance(model.dec(zs), Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objectives of choice\n",
    "import torch\n",
    "from numpy import prod\n",
    "\n",
    "from utils import log_mean_exp, is_multidata, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model(x)) #forwardは　qz_x, px_z, zs　を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (43703) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-539ff65cb3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlpx_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-539ff65cb3da>\u001b[0m in \u001b[0;36melbo\u001b[0;34m(model, x, K)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlpx_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpx_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllik_scaling\u001b[0m \u001b[0;31m#ここをBinary cross entropy で\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqz_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpz_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlpx_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (43703) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def elbo(model, x, K=1):\n",
    "    \"\"\"Computes E_{p(x)}[ELBO] \"\"\"\n",
    "    qz_x, px_z, _ = model(x)\n",
    "    lpx_z = px_z.log_prob(x).view(*px_z.batch_shape[:2], -1) * model.llik_scaling #ここをBinary cross entropy で\n",
    "    kld = kl_divergence(qz_x, model.pz(*model.pz_params))\n",
    "    return (lpx_z.sum(-1) - kld.sum(-1)).mean(0).sum()\n",
    "\n",
    "elbo(model, dataT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace(loc: torch.Size([1, 256, 43703]), scale: torch.Size([1, 256, 43703]))\n",
      "torch.Size([256, 43703])\n",
      "torch.Size([1, 256, 43703])\n",
      "torch.Size([1, 256, 43703])\n",
      "torch.Size([1, 256, 43703])\n",
      "torch.Size([256, 20])\n"
     ]
    }
   ],
   "source": [
    "x = dataT\n",
    "qz_x, px_z, _ = model(x)\n",
    "lpx_z = px_z.log_prob(x).view(*px_z.batch_shape[:2], -1) * model.llik_scaling #??\n",
    "kld = kl_divergence(qz_x, model.pz(*model.pz_params))\n",
    "\n",
    "#print((lpx_z.sum(-1) - kld.sum(-1)).mean(0).sum())\n",
    "\n",
    "print(px_z)\n",
    "print(dataT.shape)\n",
    "print(px_z.log_prob(x).shape)\n",
    "print(px_z.batch_shape)\n",
    "print(lpx_z.shape)\n",
    "print(kld.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dreg(model, x, K):\n",
    "    \"\"\"DREG estimate for log p_\\theta(x) -- fully vectorised.\"\"\"\n",
    "    _, px_z, zs = model(x, K)\n",
    "    lpz = model.pz(*model.pz_params).log_prob(zs).sum(-1)\n",
    "    lpx_z = px_z.log_prob(x).view(*px_z.batch_shape[:2], -1) * model.llik_scaling\n",
    "    qz_x = model.qz_x(*[p.detach() for p in model.qz_x_params])  # stop-grad for \\phi\n",
    "    lqz_x = qz_x.log_prob(zs).sum(-1)\n",
    "    lw = lpz + lpx_z.sum(-1) - lqz_x\n",
    "    return lw, zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_multimodal",
   "language": "python",
   "name": "venv_multimodal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
